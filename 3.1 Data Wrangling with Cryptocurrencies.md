##  What is that thing called "wrangling"?
data wrangling is the process of reshaping, aggregating, separating, or otherwise **transforming your data from one format to a more useful one.**

##  Our Aim
Run a step-forward analysis to analyze the effectiveness of a rudimentary momentum strategy that goes like this:

At the start of every month, we buy the cryptocurrency that had the largest price gain over the previous 7, 14, 21, or 28 days. We want to evaluate each of these time windows.
Then, we hold for exactly 7 days and sell our position.

## Our question
How well would we go about evaluating this strategy?

All the hard work in this task, lies in molding the data in the proper format. Once we have a proper *ABT*, answering the question becomes simple. Since this guide doesn't go deep into every subject, it is a good idea to have the [pandas library documentation](https://pandas.pydata.org/pandas-docs/stable/) open on another tab as supplemental reference. 

## 1 - Getting Our Notebook Ready
First of all, create a notebook for this tutorial.

Now import the libraries and dataset:
  
    #Pandas for managing datasets 
    import pandas as pd

Since this dataset has lots of decimals. Let's tweak the display options a bit and display floats with 2 decimals to make the tables less crowed. Let's also expand the limits for the numbers of columns and rows displayed.

    # Display floats with 2 decimal places
    pd.options.display.float_format = '{:,.2f}'.format
    
    # Expand display limits
    pd.options.display.max_rows = 200
    pd.options.display.max_columns = 100

Now [download](https://drive.google.com/file/d/1zgKTySGWDzrf4OE9Cs22ueS0_ZNg69YJ/view) the dataset and put it on the same directory as the notebook and run the following code to import and run example observations of the dataset:

    #Read BNC2 sample dataset
    df = pd.read_csv('BNC2_sample.csv',
                names = ['Code', 'Date', 'Open', 'High', 'Low',
                         'Close', 'Volume', 'VWAP', 'TWAP'])

    #Display first 5 observations
    df.head()
    
Notice how we use the *names* argument to set our own column names because the original file doesn't have any.
  - Data Directory (for code GWA_BTC):
    * Date: The day on which the index values were calculated.
    * Open: The day's opening price index for Bitcoin in US dollars.
    * High: The highest value for the price index for Bitcoin in US dollars that day.
    * Low: The lowest value for the price index for Bitcoin in US dollars that day.
    * Close: The day's closing price index for Bitcoin in US dollars.
    * Volume: The volume of Bitcoin traded that day.
    * VWAP: The volume weighted average price of Bitcoin traded that day.
    * TWAP: The time-weighted average price of Bitcoin traded that day.
    
##  2 - Understand The Data (Exploratory Phase)
A huge reason to do data wrangling, is when there is **too much information packed on a single table**, specially when dealing with time series data.

There is a **rule of thumb** that can save you from many headaches:
  * **Equivalence in Granularity:** For example, you could have 10 rows of data from 10 different cryptocurrencies. However, you should not have an 11th row with average or total values from the other 10 rows. That 11th row would be an aggregation, and thus not equivalent in granularity to the other 10.
  * **Equivalence in units:** You could have 10 rows with prices in USD collected at different dates. However, you should not then have another 10 rows with prices quoted in EUR. Any aggregations, distributions, visualizations, or statistics would become meaningless.

Our data breaks both rules. Data stored in **CSV** or **databases** tend to be in a **stacked** or **record** format. They use one single *'Code'* column as a catch all for the metadata. In our sample dataset we have the following code:

    # Unique codes in the dataset
    print( df.Code.unique() )
 
    # ['GWA_BTC' 'GWA_ETH' 'GWA_LTC' 'GWA_XLM' 'GWA_XRP' 'MWA_BTC_CNY'
    #  'MWA_BTC_EUR' 'MWA_BTC_GBP' 'MWA_BTC_JPY' 'MWA_BTC_USD' 'MWA_ETH_CNY'
    #  'MWA_ETH_EUR' 'MWA_ETH_GBP' 'MWA_ETH_JPY' 'MWA_ETH_USD' 'MWA_LTC_CNY'
    #  'MWA_LTC_EUR' 'MWA_LTC_GBP' 'MWA_LTC_JPY' 'MWA_LTC_USD' 'MWA_XLM_CNY'
    #  'MWA_XLM_EUR' 'MWA_XLM_USD' 'MWA_XRP_CNY' 'MWA_XRP_EUR' 'MWA_XRP_GBP'
    #  'MWA_XRP_JPY' 'MWA_XRP_USD']
    
According to our [documentation page](https://www.quandl.com/data/BNC2-BNC-Digital-Currency-Indexed-EOD/documentation) *GWA* and *MWA* are completely different types of indicators.
  * **_MWA_** = 'market-weighted average'. It shows regional prices. There are **multiple** *MWA* for **each** cryptocurrency, and **one** for **each** local fiat. 
  * **_GWA_** = 'global-weighted average'. It shows globally indexed prices, which translate on it being an aggregation of *MWA* and **not equivalent in granularity.**

Let's look at Bitcoin's prace on the same data to see how this is represented:

    #Example of GWA and MWA relationship:
    df[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR']) 
    & (df.Date == '2018-01-01')]  
    	    
          Code	      Date	      Open	        High	        Low	          Close	        Volume	    VWAP	        TWAP
    1371	GWA_BTC	    2018-01-01	14,505.89	    14,505.89	    13,617.46	    14,092.74	    225,906.21	14,103.18	    14,093.73
    9074	MWA_BTC_EUR	2018-01-01	11,859.35	    11,859.35	    11,111.07	    11,403.92	    14,933.73	  11,488.45	    11,478.08
    11838	MWA_BTC_JPY	2018-01-01	1,674,341.45	1,678,567.55	1,572,173.90	1,632,657.51	68,611.95	  1,632,994.40	1,631,407.66

When we call *df[df.Code.isin()]* we are just telling to the pandas library to look for observations from the dataframe *df* on the feature/variable *Code* that is in the parameters that we stablish inside our parenthesis.  
Here we can identify two clear problems with our data:
  * We have multiple entries for a cryptocurrency on a given date
  * The *MWA* is calculated on the local currency (non equivalent units, which also creates the need of historical exchange rates.
  
When we have different levels of granularity and/or different units, it becomes really hard to manage it at beast, and straigh impossible at worst. **This is why it is so important to spot equivalence problems in our exploratory stage**, in reality, it is really important to spot any kinds of problems that could damage our modelign process. 

## 3 - Filter Unwanted Observations (A.K.A. A Slice Of Data Cleaning)
Since we know that *GWA* are aggregations of *MWA*, we can infer that we only need to keep the *Global GWA* to perform our analysis:

    # Number of observations in dataset
    print( 'Before:', len(df) )
    # Before: 31761
 
    # Get all the GWA codes
    gwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]
 
    # Only keep GWA observations
    df = df[df.Code.isin(gwa_codes)]
 
    # Number of observations left
    print( 'After:', len(df) )
    # After: 6309

Now we only have the *GWA Codes* in our dataframe, which means that our observations are equivalent in granularity and  units. Therefore we can proceed.

## 4 - Pivot The Dataset (Feature Engineering)
It would be a huge pain to calculate the returns over the prior 7, 14, 21 and 28 days for the first day of each month. This would involve writing helper functions, loops, an lots of conditional logic. So we want to take a more elegant (better) approach.
  1. We'll pivot the dataset while keeping only one price column, the *VWAP* (but you could make a good case for most of them). 
     *  Pivot means that it returns a reshaped DataFrame organized by given index / column values.
     
    # Pivot dataset
    pivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')

    # Display examples from pivoted dataset
    pivoted_df.tail()

By runing this code, what we've just done is created a new dataset that contains our pivoted data. And this pivoted data is organized as the following:
  - The columns are generated in function of each class of the categorical variable *Code*.
  - The indexing of the rows is generated in function of the numerical variable *Date*.
  - Each observation shown is the data from the *VWAP* (volume weighted average price).
  
## 5 - Shift The Pivoted Dataset (+ Feaute Engineering)
To easily calculate returns over the prior 7, 14, 21, and 28 days we can use *pandas.shift()*.
The function of this method is to **shift the index** of the dataframe by *x* number of periods. Let's give it a try with 1.

    print( pivoted_df.tail(3) )
    # Code         GWA_BTC  GWA_ETH  GWA_LTC  GWA_XLM  GWA_XRP
    # Date                                                    
    # 2018-01-21 12,326.23 1,108.90   197.36     0.48     1.55
    # 2018-01-22 11,397.52 1,038.21   184.92     0.47     1.43
    # 2018-01-23 10,921.00   992.05   176.95     0.47     1.42
 
    print( pivoted_df.tail(3).shift(1) )
    # Code         GWA_BTC  GWA_ETH  GWA_LTC  GWA_XLM  GWA_XRP
    # Date                                                    
    # 2018-01-21       nan      nan      nan      nan      nan
    # 2018-01-22 12,326.23 1,108.90   197.36     0.48     1.55
    # 2018-01-23 11,397.52 1,038.21   184.92     0.47     1.43
    
Now what happened is that the shifted dataset has values from the prior day. We can take advantage of this to calculate our 7^4 day windows. First let's calculate returns over the seven prior days:

    #Calculate returns over 7 prior days
    roi7 = pivoted_df / pivoted_df.shift(7) - 1.0

    #Display examples
    roi7.tail()
    
Now we want to create a loop and write our returns on a dictionary for every time window:
    
    # Calculate returns over each window and store them in dictionary
    delta_dict = {}
    for offset in [7, 14, 21, 28]:
    delta_dict['delta_{}'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1.0
   
What we are doign here, is that we are creating objects inside our dictionary that contain a keyword named *delta_theoffsetformat{}* and that keyword has items *pivoted_df / pivoted_df.shift(offset) - 1.0.*
For this calculation to be possible we have to met two assumptions:
  1. The observations are sorted ascending by date.
  2.  There are no missing dates.

You have to confirm if this assumptions are met in order to have a succesful calculation.

## 6 - Melt The Shifted Dataset (++ Feature Engineering)
Now that we have done our calculations, we want to *melt* (unpivot) the data so we can later create a *ABT* where each row contains all of the relevant information for a particular coin on a particular date.

If we try to do this with our original dataset it would result on an error because since the data for different coins is stacked on each other, then the boundaries would be overlapped. In other words, BTC data would be in ETH calculations and ETH data on LTC calculation and so on... The code for melting the data for one dataframe is the following:

    #Melt roi7 returns 
    melted_roi7 = roi7.reset_index().melt(id_vars=['Date'], value_name = 'roi7')

    #Melted dataframe examples
    melted_roi7.tail()
    
![Melt roi7 return](https://elitedatascience.com/wp-content/uploads/2018/01/Melted-Crypto-Dataset-Examples.jpg)
    
What we've done to melt the data is:
  1.  Use the *reset_index()* method so we can call the columns by their respective original name.
  2.  Call the *melt()* method to unpivot the data.
  3.  Pass the columns to be maintained into the order of *id_vars = argument*.
  4.  Name the melted colum using the *value_name = argument*.
  
To do this for all our dataframes, we just need to loop through *delta_dic* like the following:
    
    # Melt all the delta dataframes and store in list
    melted_dfs = []
    for key, delta_df in delta_dict.items():
    melted_dfs.append( delta_df.reset_index().melt(id_vars=['Date'], value_name=key) )
    
What the loop states is that for every key (arbitrarily named *delta_df*) inside the dictionary, append to *melted_dfs* list, the melted ROI of every offset-key (which remember, was arbitrarily named *delta_df*). 
    
Now, to do this but with the forward looking 7 day returns (for our target to evaluate our strategy). We just have to change our shift number to a negative one, so it shifts the numbers backwards instead of forward. Essentially  to predict the future.
    
    # Calculate 7-day returns after the date
    return_df = pivoted_df.shift(-7) / pivoted_df - 1.0
 
    # Melt the return dataset and append to list
    melted_dfs.append( return_df.reset_index().melt(id_vars=['Date'], value_name='return_7') )

We now have 5 melted dataframes stored in the  melted_dfs list, one for each of the backward-looking 7, 14, 21, and 28-day returns and one for the forward-looking 7-day returns.

## 7 - Reduce-Merge The Melted Data (Guess The Stage)
All that's left is to join all the dataframes that we've melted, into a single *ABT*. For this, we'll need two tools:

  1.  Panda merge function that works like *SQL JOIN*. 

    #Merge two dataframes
    pd.merge(melted_dfs[0], melted_dfs[1], on = ['Date', 'Code']).tail()
    
![TwoDfsMelted](https://elitedatascience.com/wp-content/uploads/2018/01/Merged-Crypto-Dataset-Examples.jpg)
  
Here you see that *delta_7* and *delta_14* are side by side in the features and in the same row. That's how **our ABT starts.** Now what we want to do is merge the other melted dataframes with a base dataframe of **other features** that we might want.

The **best** way to do this is the python biuld-in function *reduce*. 

    from functools import reduce
    
Before using that function, let's create a *feature_dfs* list that contains base features from our original dataframe, combined with our melte dataframes.

    #Grab features from original dataset
    base_df = df[['Date', 'Code', 'Volume', 'VWAP']]

    #Create a list with all the feature dataframes
    feature_dfs = [base_df] + melted_dfs
    
Now we can call our reduce-merge functions on all our features like:

    #Reduce-merge features into analytical base table
    abt = reduce(lambda left,right: pd.merge(left,right,on=['Date', 'Code']), feature_dfs)

    #Display example from the abt
    abt.tail(10)
    
![abt](https://elitedatascience.com/wp-content/uploads/2018/01/ABT-Crypto-Dataset-Examples.jpg)

Here you see that what *reduce* does, is that first, it applies a function of two arguments **cumulatively** to the objects in a **sequence** (i.e. a,b: a+b, [2,4,6,8,10] this calculates into ((((2+4)+6)+8)+10) This is how our columns get reduced into the abt. it sums side by side the features (that's why the left, right names). What it does is first merge the *Date* and *Code* features, and then sums all the others like ((((['Date', 'Code'] + 1st_feature_dfs_item) + 2nd_feature_dfs_item) + 3rd_feat... last_feature_dfs_item)

Data Dictionary for our Analytical Base Table (ABT):
  - Date: The day on which the index values were calculated.
  - Code: Which cryptocurrency.
  - VWAP: The volume weighted average price traded that day.
  - delta_7: Return over the prior 7 days (1.0 = 100% return).
  - delta_14: Return over the prior 14 days (1.0 = 100% return).
  - delta_21: Return over the prior 21 days (1.0 = 100% return).
  - delta_28: Return over the prior 28 days (1.0 = 100% return).
  - return_7: Future return over the next 7 days (1.0 = 100% return).
  
The last 7 observations of the *'roi7'* feature don't have value because those don't have a 7th day of data from which do its calculations. 

With this table we can solve our questions.

Want to pick the coin that had the biggest momentum on september 2017? Display the rows for that date and look at it's prior returns. 

    #Data from september 1st, 2017
    abt[abt.Date == '2017-09-01']
    
If you want to do the calculations with python instead of reasoning it by yourself:

    max_momentum_id = abt[abt.Date == '2017-09-01'].delta_28.idxmax()
    abt.loc[max_momentum_id, ['Code','roi7']]
    #Code    GWA_LTC
    #roi7      -0.10
    #Name: 3543, dtype: object
    
## 8 - Aggregate with group-by (More Engineering (But This Is Optional))
This is **if** we want to only keep the first days of each month. We can use *group-by* followed by *aggregation*.
  1. Create a new *'month'* feature from the first 7 characters of the *Date* strings.
  2. Then, **group** the observations by *'Code'* and *'month'*. With this, panda will create cells of data that separate observations by *'Code'* and *'month'*.
  3. With each group, take the *.first()* observation and reset the index.
     -  All of this works properly **if your df is sorted by date.**

Now we have a *ABT* with the following features:
  * Only relevant data from the 1st day of each month.
  * Momentum features calculated from the prior 7, 14, 21, and 28 days.
  * The future returns you would've made 7 days later.
  
### We introduced several key tools for filtering, manipulating, and transforming datasets in Python, but we've only scratched the surface. Pandas is a very powerful library with plenty of additional functionality.
### Now download more datasets for hands-on practice. Propose an interesting question, plan your approach, and fall back on documentation for help.
