##  What is that thing called "wrangling"?
data wrangling is the process of reshaping, aggregating, separating, or otherwise **transforming your data from one format to a more useful one.**

##  Our Aim
Run a step-forward analysis to analyze the effectiveness of a rudimentary momentum strategy that goes like this:

At the start of every month, we buy the cryptocurrency that had the largest price gain over the previous 7, 14, 21, or 28 days. We want to evaluate each of these time windows.
Then, we hold for exactly 7 days and sell our position.

## Our question
How well would we go about evaluating this strategy?

All the hard work in this task, lies in molding the data in the proper format. Once we have a proper *ABT*, answering the question becomes simple. Since this guide doesn't go deep into every subject, it is a good idea to have the [pandas library documentation](https://pandas.pydata.org/pandas-docs/stable/) open on another tab as supplemental reference. 

## 1 - Getting Our Notebook Ready
First of all, create a notebook for this tutorial.

Now import the libraries and dataset:
  
    #Pandas for managing datasets 
    import pandas as pd

Since this dataset has lots of decimals. Let's tweak the display options a bit and display floats with 2 decimals to make the tables less crowed. Let's also expand the limits for the numbers of columns and rows displayed.

    # Display floats with 2 decimal places
    pd.options.display.float_format = '{:,.2f}'.format
    
    # Expand display limits
    pd.options.display.max_rows = 200
    pd.options.display.max_columns = 100

Now [download](https://drive.google.com/file/d/1zgKTySGWDzrf4OE9Cs22ueS0_ZNg69YJ/view) the dataset and put it on the same directory as the notebook and run the following code to import and run example observations of the dataset:

    #Read BNC2 sample dataset
    df = pd.read_csv('BNC2_sample.csv',
                names = ['Code', 'Date', 'Open', 'High', 'Low',
                         'Close', 'Volume', 'VWAP', 'TWAP'])

    #Display first 5 observations
    df.head()
    
Notice how we use the *names* argument to set our own column names because the original file doesn't have any.
  - Data Directory (for code GWA_BTC):
    * Date: The day on which the index values were calculated.
    * Open: The day's opening price index for Bitcoin in US dollars.
    * High: The highest value for the price index for Bitcoin in US dollars that day.
    * Low: The lowest value for the price index for Bitcoin in US dollars that day.
    * Close: The day's closing price index for Bitcoin in US dollars.
    * Volume: The volume of Bitcoin traded that day.
    * VWAP: The volume weighted average price of Bitcoin traded that day.
    * TWAP: The time-weighted average price of Bitcoin traded that day.
    
##  2 - Understand The Data (Exploratory Phase)
A huge reason to do data wrangling, is when there is **too much information packed on a single table**, specially when dealing with time series data.

There is a **rule of thumb** that can save you from many headaches:
  * **Equivalence in Granularity:** For example, you could have 10 rows of data from 10 different cryptocurrencies. However, you should not have an 11th row with average or total values from the other 10 rows. That 11th row would be an aggregation, and thus not equivalent in granularity to the other 10.
  * **Equivalence in units:** You could have 10 rows with prices in USD collected at different dates. However, you should not then have another 10 rows with prices quoted in EUR. Any aggregations, distributions, visualizations, or statistics would become meaningless.

Our data breaks both rules. Data stored in **CSV** or **databases** tend to be in a **stacked** or **record** format. They use one single *'Code'* column as a catch all for the metadata. In our sample dataset we have the following code:

    # Unique codes in the dataset
    print( df.Code.unique() )
 
    # ['GWA_BTC' 'GWA_ETH' 'GWA_LTC' 'GWA_XLM' 'GWA_XRP' 'MWA_BTC_CNY'
    #  'MWA_BTC_EUR' 'MWA_BTC_GBP' 'MWA_BTC_JPY' 'MWA_BTC_USD' 'MWA_ETH_CNY'
    #  'MWA_ETH_EUR' 'MWA_ETH_GBP' 'MWA_ETH_JPY' 'MWA_ETH_USD' 'MWA_LTC_CNY'
    #  'MWA_LTC_EUR' 'MWA_LTC_GBP' 'MWA_LTC_JPY' 'MWA_LTC_USD' 'MWA_XLM_CNY'
    #  'MWA_XLM_EUR' 'MWA_XLM_USD' 'MWA_XRP_CNY' 'MWA_XRP_EUR' 'MWA_XRP_GBP'
    #  'MWA_XRP_JPY' 'MWA_XRP_USD']
    
According to our [documentation page](https://www.quandl.com/data/BNC2-BNC-Digital-Currency-Indexed-EOD/documentation) *GWA* and *MWA* are completely different types of indicators.
  * **_MWA_** = 'market-weighted average'. It shows regional prices. There are **multiple** *MWA* for **each** cryptocurrency, and **one** for **each** local fiat. 
  * **_GWA_** = 'global-weighted average'. It shows globally indexed prices, which translate on it being an aggregation of *MWA* and **not equivalent in granularity.**

Let's look at Bitcoin's prace on the same data to see how this is represented:

    #Example of GWA and MWA relationship:
    df[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR']) 
    & (df.Date == '2018-01-01')]  
    	    
          Code	      Date	      Open	        High	        Low	          Close	        Volume	    VWAP	        TWAP
    1371	GWA_BTC	    2018-01-01	14,505.89	    14,505.89	    13,617.46	    14,092.74	    225,906.21	14,103.18	    14,093.73
    9074	MWA_BTC_EUR	2018-01-01	11,859.35	    11,859.35	    11,111.07	    11,403.92	    14,933.73	  11,488.45	    11,478.08
    11838	MWA_BTC_JPY	2018-01-01	1,674,341.45	1,678,567.55	1,572,173.90	1,632,657.51	68,611.95	  1,632,994.40	1,631,407.66

When we call *df[df.Code.isin()]* we are just telling to the pandas library to look for observations from the dataframe *df* on the feature/variable *Code* that is in the parameters that we stablish inside our parenthesis.  
Here we can identify two clear problems with our data:
  * We have multiple entries for a cryptocurrency on a given date
  * The *MWA* is calculated on the local currency (non equivalent units, which also creates the need of historical exchange rates.
  
When we have different levels of granularity and/or different units, it becomes really hard to manage it at beast, and straigh impossible at worst. **This is why it is so important to spot equivalence problems in our exploratory stage**, in reality, it is really important to spot any kinds of problems that could damage our modelign process. 

## 3 - Filter Unwanted Observations (A.K.A. A Slice Of Data Cleaning)
Since we know that *GWA* are aggregations of *MWA*, we can infer that we only need to keep the *Global GWA* to perform our analysis:

    # Number of observations in dataset
    print( 'Before:', len(df) )
    # Before: 31761
 
    # Get all the GWA codes
    gwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]
 
    # Only keep GWA observations
    df = df[df.Code.isin(gwa_codes)]
 
    # Number of observations left
    print( 'After:', len(df) )
    # After: 6309

Now we only have the *GWA Codes* in our dataframe, which means that our observations are equivalent in granularity and  units. Therefore we can proceed.

## 4 - Pivot The Dataset (Feature Engineering)
It would be a huge pain to calculate the returns over the prior 7, 14, 21 and 28 days for the first day of each month. This would involve writing helper functions, loops, an lots of conditional logic. So we want to take a more elegant (better) approach.
  1. We'll pivot the dataset while keeping only one price column, the *VWAP* (but you could make a good case for most of them). 
     *  Pivot means that it returns a reshaped DataFrame organized by given index / column values.
     
    # Pivot dataset
    pivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')

    # Display examples from pivoted dataset
    pivoted_df.tail()

By runing this code, what we've just done is created a new dataset that contains our pivoted data. And this pivoted data is organized as the following:
  - The columns are generated in function of each class of the categorical variable Code.
  - The indexing of the rows is generated in function of the numerical variable Date.
  - Each observation shown is the data from the *VWAP* (volume weighted average price).
  
