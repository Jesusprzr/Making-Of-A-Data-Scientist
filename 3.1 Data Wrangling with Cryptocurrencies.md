##  What is that thing called "wrangling"?
data wrangling is the process of reshaping, aggregating, separating, or otherwise **transforming your data from one format to a more useful one.**

##  Our Aim
Run a step-forward analysis to analyze the effectiveness of a rudimentary momentum strategy that goes like this:

At the start of every month, we buy the cryptocurrency that had the largest price gain over the previous 7, 14, 21, or 28 days. We want to evaluate each of these time windows.
Then, we hold for exactly 7 days and sell our position.

## Our question
How well would we go about evaluating this strategy?

All the hard work in this task, lies in molding the data in the proper format. Once we have a proper *ABT*, answering the question becomes simple. Since this guide doesn't go deep into every subject, it is a good idea to have the [pandas library documentation](https://pandas.pydata.org/pandas-docs/stable/) open on another tab as supplemental reference. 

## 1 - Getting Our Notebook Ready
First of all, create a notebook for this tutorial.

Now import the libraries and dataset:
  
    #Pandas for managing datasets 
    import pandas as pd

Since this dataset has lots of decimals. Let's tweak the display options a bit and display floats with 2 decimals to make the tables less crowed. Let's also expand the limits for the numbers of columns and rows displayed.

    # Display floats with 2 decimal places
    pd.options.display.float_format = '{:,.2f}'.format
    
    # Expand display limits
    pd.options.display.max_rows = 200
    pd.options.display.max_columns = 100

Now [download](https://drive.google.com/file/d/1zgKTySGWDzrf4OE9Cs22ueS0_ZNg69YJ/view) the dataset and put it on the same directory as the notebook and run the following code to import and run example observations of the dataset:

    #Read BNC2 sample dataset
    df = pd.read_csv('BNC2_sample.csv',
                names = ['Code', 'Date', 'Open', 'High', 'Low',
                         'Close', 'Volume', 'VWAP', 'TWAP'])

    #Display first 5 observations
    df.head()
    
Notice how we use the *names* argument to set our own column names because the original file doesn't have any.
  - Data Directory (for code GWA_BTC):
    * Date: The day on which the index values were calculated.
    * Open: The day's opening price index for Bitcoin in US dollars.
    * High: The highest value for the price index for Bitcoin in US dollars that day.
    * Low: The lowest value for the price index for Bitcoin in US dollars that day.
    * Close: The day's closing price index for Bitcoin in US dollars.
    * Volume: The volume of Bitcoin traded that day.
    * VWAP: The volume weighted average price of Bitcoin traded that day.
    * TWAP: The time-weighted average price of Bitcoin traded that day.
    
##  2 - Understand The Data (Exploratory Phase)
A huge reason to do data wrangling, is when there is **too much information packed on a single table**, specially when dealing with time series data.

There is a **rule of thumb** that can save you from many headaches:
  * **Equivalence in Granularity:** For example, you could have 10 rows of data from 10 different cryptocurrencies. However, you should not have an 11th row with average or total values from the other 10 rows. That 11th row would be an aggregation, and thus not equivalent in granularity to the other 10.
  * **Equivalence in units:** You could have 10 rows with prices in USD collected at different dates. However, you should not then have another 10 rows with prices quoted in EUR. Any aggregations, distributions, visualizations, or statistics would become meaningless.

Our data breaks both rules. Data stored in **CSV** or **databases** tend to be in a **stacked** or **record** format. They use one single *'Code'* column as a catch all for the metadata. In our sample dataset we have the following code:

    # Unique codes in the dataset
    print( df.Code.unique() )
 
    # ['GWA_BTC' 'GWA_ETH' 'GWA_LTC' 'GWA_XLM' 'GWA_XRP' 'MWA_BTC_CNY'
    #  'MWA_BTC_EUR' 'MWA_BTC_GBP' 'MWA_BTC_JPY' 'MWA_BTC_USD' 'MWA_ETH_CNY'
    #  'MWA_ETH_EUR' 'MWA_ETH_GBP' 'MWA_ETH_JPY' 'MWA_ETH_USD' 'MWA_LTC_CNY'
    #  'MWA_LTC_EUR' 'MWA_LTC_GBP' 'MWA_LTC_JPY' 'MWA_LTC_USD' 'MWA_XLM_CNY'
    #  'MWA_XLM_EUR' 'MWA_XLM_USD' 'MWA_XRP_CNY' 'MWA_XRP_EUR' 'MWA_XRP_GBP'
    #  'MWA_XRP_JPY' 'MWA_XRP_USD']
    
According to our [documentation page](https://www.quandl.com/data/BNC2-BNC-Digital-Currency-Indexed-EOD/documentation) *GWA* and *MWA* are completely different types of indicators.
  * **_MWA_** = 'market-weighted average'. It shows regional prices. There are **multiple** *MWA* for **each** cryptocurrency, and **one** for **each** local fiat. 
  * **_GWA_** = 'global-weighted average'. It shows globally indexed prices, which translate on it being an aggregation of *MWA* and **not equivalent in granularity.**

Let's look at Bitcoin's prace on the same data to see how this is represented:

    #Example of GWA and MWA relationship:
    df[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR']) 
    & (df.Date == '2018-01-01')]  
    	    
          Code	      Date	      Open	        High	        Low	          Close	        Volume	    VWAP	        TWAP
    1371	GWA_BTC	    2018-01-01	14,505.89	    14,505.89	    13,617.46	    14,092.74	    225,906.21	14,103.18	    14,093.73
    9074	MWA_BTC_EUR	2018-01-01	11,859.35	    11,859.35	    11,111.07	    11,403.92	    14,933.73	  11,488.45	    11,478.08
    11838	MWA_BTC_JPY	2018-01-01	1,674,341.45	1,678,567.55	1,572,173.90	1,632,657.51	68,611.95	  1,632,994.40	1,631,407.66

When we call *df[df.Code.isin()]* we are just telling to the pandas library to look for observations from the dataframe *df* on the feature/variable *Code* that is in the parameters that we stablish inside our parenthesis.  
Here we can identify two clear problems with our data:
  * We have multiple entries for a cryptocurrency on a given date
  * The *MWA* is calculated on the local currency (non equivalent units, which also creates the need of historical exchange rates.
  
When we have different levels of granularity and/or different units, it becomes really hard to manage it at beast, and straigh impossible at worst. **This is why it is so important to spot equivalence problems in our exploratory stage**, in reality, it is really important to spot any kinds of problems that could damage our modelign process. 

## 3 - Filter Unwanted Observations (A.K.A. A Slice Of Data Cleaning)
Since we know that *GWA* are aggregations of *MWA*, we can infer that we only need to keep the *Global GWA* to perform our analysis:

    # Number of observations in dataset
    print( 'Before:', len(df) )
    # Before: 31761
 
    # Get all the GWA codes
    gwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]
 
    # Only keep GWA observations
    df = df[df.Code.isin(gwa_codes)]
 
    # Number of observations left
    print( 'After:', len(df) )
    # After: 6309

Now we only have the *GWA Codes* in our dataframe, which means that our observations are equivalent in granularity and  units. Therefore we can proceed.

## 4 - Pivot The Dataset (Feature Engineering)
It would be a huge pain to calculate the returns over the prior 7, 14, 21 and 28 days for the first day of each month. This would involve writing helper functions, loops, an lots of conditional logic. So we want to take a more elegant (better) approach.
  1. We'll pivot the dataset while keeping only one price column, the *VWAP* (but you could make a good case for most of them). 
     *  Pivot means that it returns a reshaped DataFrame organized by given index / column values.
     
    # Pivot dataset
    pivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')

    # Display examples from pivoted dataset
    pivoted_df.tail()

By runing this code, what we've just done is created a new dataset that contains our pivoted data. And this pivoted data is organized as the following:
  - The columns are generated in function of each class of the categorical variable *Code*.
  - The indexing of the rows is generated in function of the numerical variable *Date*.
  - Each observation shown is the data from the *VWAP* (volume weighted average price).
  
## 5 - Shift The Pivoted Dataset (+ Feaute Engineering)
To easily calculate returns over the prior 7, 14, 21, and 28 days we can use *pandas.shift()*.
The function of this method is to **shift the index** of the dataframe by *x* number of periods. Let's give it a try with 1.

    print( pivoted_df.tail(3) )
    # Code         GWA_BTC  GWA_ETH  GWA_LTC  GWA_XLM  GWA_XRP
    # Date                                                    
    # 2018-01-21 12,326.23 1,108.90   197.36     0.48     1.55
    # 2018-01-22 11,397.52 1,038.21   184.92     0.47     1.43
    # 2018-01-23 10,921.00   992.05   176.95     0.47     1.42
 
    print( pivoted_df.tail(3).shift(1) )
    # Code         GWA_BTC  GWA_ETH  GWA_LTC  GWA_XLM  GWA_XRP
    # Date                                                    
    # 2018-01-21       nan      nan      nan      nan      nan
    # 2018-01-22 12,326.23 1,108.90   197.36     0.48     1.55
    # 2018-01-23 11,397.52 1,038.21   184.92     0.47     1.43
    
Now what happened is that the shifted dataset has values from the prior day. We can take advantage of this to calculate our 7^4 day windows. First let's calculate returns over the seven prior days:

    # Calculate returns over 7 days prior
    delta_7 = pivoted_df / pivoted_df.shift(7) - 1.0
 
    # Display examples
    delta_7.tail()
    
Now we want to create a loop and write our returns on a dictionary:
    
     # Calculate returns over each window and store them in dictionary
    delta_dict = {}
    for offset in [7, 14, 21, 28]:
    delta_dict['delta_{}'.format(offset)] = pivoted_df / pivoted_df.shift(offset) - 1.0
    
For this calculation to be possible we have to met two assumptions:
  1. The observations are sorted ascending by date.
  2.  There are no missing dates.

You have to confirm if this assumptions are met in order to have a succesful calculation.

## 7 - Melt The Shifted Dataset
Now that we have done our calculations, we want to *melt* (unpivot) the data so we can later create a *ABT* where each row contains all of the relevant information for a particular coin on a particular date.

If we try to do this with our original dataset it would result on an error because since the data for different coins is stacked on each other, then the boundaries would be overlapped. In other words, BTC data would be in ETH calculations and ETH data on LTC calculation and so on... The code for melting the data for one dataframe is the following:

    #Melt roy7 returns 
    melted_roi7 = roi7.reset_index().melt(id_vars=['Date'], value_name = 'roi7')

    #Melted dataframe examples
    melted_roi7.tail()
    
What we've done to melt the data is:
  1.  Use the *reset_index()* method so we can call the columns by name.
  2.  Call the *melt()* method to unpivot the data.
  3.  Pass the columns to be maintained into the *id_vars = argument*.
  4.  Name the melted colums using the *value_name = argument*.
  
To do this for all our dataframes, we just need to loop through *delta_dic* like the following:
    
    # Melt all the delta dataframes and store in list
    melted_dfs = []
    for key, delta_df in delta_dict.items():
    melted_dfs.append( delta_df.reset_index().melt(id_vars=['Date'], value_name=key) )
    
Now, to do this but with the forward looking 7 day returns (for our target to evaluate our strategy). We just have to change our shift number to a negative one, pretty intuitive isn't it?
    
    # Calculate 7-day returns after the date
    return_df = pivoted_df.shift(-7) / pivoted_df - 1.0
 
    # Melt the return dataset and append to list
    melted_dfs.append( return_df.reset_index().melt(id_vars=['Date'], value_name='return_7') )

We now have 5 melted dataframes stored in the  melted_dfs list, one for each of the backward-looking 7, 14, 21, and 28-day returns and one for the forward-looking 7-day returns.
