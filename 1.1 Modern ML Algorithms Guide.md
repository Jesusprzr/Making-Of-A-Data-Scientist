# Modern Machine Learning Algorithms
We will categorize our algorithms by *ML task*. This approach is focused on what's usually the end goal of all. Which is the expected result! It could be something like predicting an outcome or classifying our observations.

##  No Free Lunch Theorem
This is a theorem that states that no algorithm works best for every problem. This is specially relevant for supervised learning (i.e. predictive modeling). There are many factors at play to already state which algorithm is the best. This is why you should **try many different algirithms for your problem**. Of course, the algorithms you try should be appropiated for your problem, and that's when picking the right machine learning task comes in.

## Machine Learning Tasks
Here we'll cover the big three:
  1. Regression
  2. Classification
  3. Clustering
  
Another think to take in count:
  * We will not cover domain specific adaptations.
  * We will not cover every algorithm. What this list will give you is representative overview of successful contemporary algorithms for each task.

### 1 - Regression
Is the supervised learning task for modeling and predicting **continuos, numeric** variables.
These tasks are caracterized for having **labeled datasets that have a numeric target variable.**
So you have values of each observation that you can use to **supervise** your algorithm.

#### 1.1 - (Regularized) Linear Regression
It attemps to fit a straight hyperplane or line (when have only two vars). It works well when there is a linear relationship between your variables.

In practice simple linear regression is simply outcassed by its regularized versions (LASSO, Ridge, and Elastic-Net).
Regularization is a technique utilized for penalizing large coefficients in order to avoid overfitting, and the strenght of the penalty should be tuned.
  * **Strenghts:** Is straightforward to understand and explain, and **can** be regularized to avoid overfitting. It also can be updated easily with new data using **stochastic gradient descent.**
  * **Weaknesses:** Performs poorly when there is non-linear relationship. It is **not** naturally flexible enough to capture more compplex patterns. Also adding the right interaction terms of polynomials can be tricky and time-consuming.
  * **Implementations:** Python/R
  
#### 1.2 - Regression Trees (Ensembles)
A.K.A. Decision trees, a tree like branching structure that allows it to fit non-linear relationships.

Ensamble method, such as **Random Forests (RF)** and **Gradient Boosted Trees (GBM)**, combine predictions from many individual trees. **RF's** perform very well out of the box, while **GBM's** are harder to tune but tend to have **higher performance ceilings.**
  * **Strenghts:** They **can** learn non-linear relationships and are fairly **robust** to outliers. Ensembles perform really well in practice! They win lots of *ML* competitions (non-deep-learning).
  * **Weaknesses:** Unconstrained, individual trees are prone to overfitting because they **can keep branching** until they memorize the training data. <- This can ve alleviated by using **ensembles.**
  * **Implementations:** RF: Python/R GBM: Python/R
#### 1.3 - Deep Learning
**Multi-layer neural networks** that can learn extremely complex patterns.
They use "hidden layers" between inputs and outputs in order to model **intermediary representations** of the data that other olgarithms can't easily learn.

They can learn from high-dimensional data thanks the mechanisms they have (such as convolutions and drop-out). The pitfall is that it requires so much more data than other algorithms to train. Because the model has orders of magnitudes, so more parameters to stimate.
  * **Strenghts:** Literally current state of the art for certain domains. They perform really well on computer vision, speech recognition, image, audio, and text data. They can be easily updated with new data using **batch propagation.** Their architectures can be adapted to many types of problems, and their **hidden layers** reduce the need for feature engineering.
  * **Weaknesses:** Usually **not** suitable as general purpose algorithms because they require a really large amount of data. For **classical ML** are usually outperformed by **tree ensembles.** Another thing is that they are computationally heavy to train and require a lot mor eexpertise to tune. 
  * **Implementations:** Python/R
#### 1.4 - Honorable Mention: Nearest Neighbors
These are **instance-based** algorithms = they save each training observation. They then make predictions for new observations by searching for the most similar training observations and pooling their values.

Since this algoritms are memory-intensive, they tend to perform bad in high-dimensional data, and require a meaningful distance function to calculate similarity. This is why in practice training regularized regression or tree ensembles are almot always better uses of your time. 

### 2 - Classification
Is like **regression**, the supervised learning task **BUT** this one is used for predicting **categorical variables.**
Now you'll see how many **regression** algorithms have its categorical counterparts. The algorithms are adapted to predict descriptive variables (a class) intead of quantitative ones.

#### 2.1 - (Regularized) logistic regression
As you already guessed, is the classification counterpart of **linear regression.** Here we map predictions by 0 and 1 (binary), which means that we can interpret them as class probabilities.

This models themselves are still linear, so they work well when your classes are linearly separable (i.e. separated by a single decision surface). As **linear**, **logistic regression** can be regularized by penalizing coefficients with a tuneable penalty strenght.
  * **Strenghts:** Outputs have a nice probabilistic interpretation, and we can regularize the algo to avoid overfitting. It can be easily updated with stochastic gradient descent.
  * **Weaknesses:** It tends to underperform when there is are multiple or non-linear decision boundaries. Since they are not much flexible, they can't capture more complex relationships naturally.  
  * **Implementations:** Python/R

#### 2.2 - Classification Trees (Ensembles)
Counterpart of **regression trees.** Both are called **"decision trees"** or **"classification and regression trees (CART)"**.
  * **Strenghts:** Perform really well in practice. Robust to outliers, scalable, and able to naturally model non-linear relatinships, which allows them to train on more complex patterns.
  * **Weaknesses:** Unconstrained, individual trees proned to overfitting, alleviated by **ensemble**. 
  * **Implementations:** RF: Python/R GBM: Python/R

#### 2.3 - Deep Learning
Easily adapted to classification problems. This is often the **most common use of deep learning.** 
  * **Strenghts:** performs very well when classifying for audio, text, and image data.
  * **Weaknesses:** require very large amounts of data to train, so it's not treated as a general-purpose algorithm.
  * **Implementations:** Python/R

#### 2.4 - Support Vector Machines
**SVM** use a mechanism called **Kernels**, which essentially calculate distance between two observations. The **SVM** algorithm then finds a decision boundary that maximizes the distance between the closest members of separated classes.

an **SVM** with a linear **kernel** is similar to logistic regression. Therefore, in practice, the benefit of **SVM's** typically comes from using **non-linear kernels** to model **non-linear decision boundaries.**
  * **Strenghts:** They can model non-linear decision boundaries, and there are many **kernels** to choose from. Fairly  robust againts overfitting, specially in **high-dimensional space.**
  * **Weaknesses:** Memory intensive, trickier to tune due to the importance of picking the right kernel and as you should have guessed, don't scale well to large datasets. Currently **RF's** are preferred over **SVM's.**  
  * **Implementations:** Python/R

#### 2.5 - Naive Bayes
**NB**, very simple algorithm based on conditional probability and counting. Is essentially a probability table that gets updated through your training data. To predict a new observation, you'll simply "look up" the class probabilities in your probability table based on its feature values. 

It's called "naive" because its core assumption of conditional independence (i.e. all input features are independent from one another) rarely holds true in the real world.
  * **Strenghts:** For our surprise, besides of the fact that conditional independence rarely holds true, the model actually performs really well in practice. This is specially thanks to its simplicity. They are easy to implement and can scalle with your dataset.
  * **Weaknesses:** Due to its simplicity, these models are often outperformed by properly trained and tuned models usign the previous algorithms listed.
  * **Implementations:** Python/R
  
### 3 - Clustering
Unsupervised learning task for finding natural groupings of observations (i.e. clusters) based on the inherent structure within the dataset. 
Because it is unsupervised, there is no "right answer", data visualization is usually used to evaluated the results. If there is a "right answer" (i.e. prelabed clusters), then classification algorithms are more appropiate.

#### 3.1 - K-Means
General pusporse algo taht makes clusters based on geometric distance between points. The clusters are grouped around **centroids**, causing them to be globular and have same sizes. This alg is **recommended for begginers.** 
  * **Strenghts:** Is the most popular clustering algorithm because its fast, simple, and surprisingly flexible if you pre-process your data an engineer useful features.
  * **Weaknesses:** The user **must** specify the mumber of clusters, something that won't be easy to do all the time. Another thing is that if the true underlying clusters in your data are **not globular**, the **K-Means** will produce poor clusters. 
  * **Implementations:** Python/R
  
#### 3.2 - Afinity Propagation
Makes clusters based on graph distances between points. The clusters tend to be smaller and have uneven sizes.
  * **Strenghts:** The user **doesn't** need to specify the number of clusters (but does need to specify 'sample preference' and 'damping' hyperparameters).
  * **Weaknesses:** Its quite slow and memory-heavy, which makes it difficult to scale to larger datasets. Also it assumes the true underlying clusters are globular.
  * **Implementations:** Python/R

#### 3.3 - Hierarchical/Agglomerative
Is a suite of algorithms based on the same idea: (1) start with each point on its own custer; (2) for each cluster, merge it with other based on some criterion; (3) repeat until only one cluster remains and you are left with a **hierarchy** of clusters.
  * **Strenghts:** The clusters are not assumed to be globular. In addition, it scales well to larger datasets.
  * **Weaknesses:** the user must choose the number of clusters.
  * **Implementations:** Python/R

#### 3.4 - DBSCAN
Density based algorithm. It makes clusters for dense regions of points. There's also HDBSCAN. This one allows varying density clusters.
  * **Strenghts:** It **does not** assume globular clusters, and its performance is scalable. It **doesn't** require **every point** to be assigned to a cluster, reducing the noisy of the cluster (depending on the case this last one may be a weakness).
  * **Weaknesses:** The user must tune the hyperparameters 'epsilon' and 'min_samples,' which define the density of clusters. DBSCAN is quite sensitive to these hyperparameters.
  * **Implementations:** Python/R

## Parting Words
We just had an overview of modern algorithms for "the big 3": **regression, classification, and clustering.**
Next, we are going to look at algorithms for **dimensionality reduction**, incluiding **feature selection** and **feature extraction.**
But first, don't ever forget the following advide that comes from personal experience:
  1.  **Practice, practice, practice.** TRUE mastery comes with practice! As you practice, you develop practical intuition which **unlocks** the ability to implement almost any algorithm effectively.
  2.  **Master the fundamentals.** Almost every algorithm other algorithm is some quind of adaptation of the ones listed here. So master the ones in here, they will provide you with a strong foundation for applied machine learning.
  3.  **Better Data > Fancier Algorithms.** Remember to think of algorithms as comodities, because you can easily switch them in and out. **BUT**, effective exploratory analysis, data cleaning, and feature engineering can significantly boost your results, they are actually critical! Remember that *output = input.*
