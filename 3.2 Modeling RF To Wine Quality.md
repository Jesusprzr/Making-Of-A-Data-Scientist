## Introduction
The following modeling process will be concentrated on training and tuning a random forest algorithm for wine quality. Based on traits like acidity, residual sugar, and alcohol concentration. This will be focused on applied machine learning. 

##Why Scickit-Learn?
This is the most premier general-purpose python library for machine learning. Certainly there are others packages that perform better at certain tasks, but the strong point of this one is its versatility and high-level interface for tasks like preprocessing data, cross-validation and stuf that allows us to practice the entire *ML* workflow with good practices and understand the big picture. 

Keep the [Scikit documentation](https://scikit-learn.org/stable/user_guide.html) open on another tab for suplemental reference. 

## 1- Creating our environment
### Create a new jupyter notebook, import libraries and data:

    #Numpy for more efficient numerical computation
    import numpy as np
    #Pandas for more ease on dataframe management
    import pandas as pd

    #Machine learning functions:
    #this module helps choose between models, we're just gonna import one function first
    from sklearn.model_selection import train_test_split
    #the following module for scaling, transforming, and wrangling data
    from sklearn import preprocessing
    #Now the families of models we'll use
    from sklearn.ensemble import RandomForestRegressor 
    #For doing cross-validation
    from sklearn.pipeline import make_pipeline
    from sklearn.model_selection import GridSearchCV
    #Metrics to evaluate model performance later on
    from sklearn.metrics import mean_squared_error, r2_score
    #Finally, a way to persist our model for future use
    from sklearn.externals import joblib
    
Load the dataset **from a remote URL:**

    dURL = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
    ds = pd.read_csv(dURL)
    
    print(ds.head())
      fixed acidity;"volatile acidity";"citric acid";"residual sugar";"chlorides";"free sulfur dioxide";"total sulfur             dioxide";"density";"pH";"sulphates";"alcohol";"quality"
    0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                     1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                     2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                     3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                     4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5  
    
Looks pretty messy, it seems like the file is actually using semicolons (;) to actually separate the data. That's easilly fixed with:

    ds = pd.read_csv(dataset_url, sep=';')

    print data.head()
    #    fixed acidity  volatile acidity  citric acid...
    # 0            7.4              0.70         0.00...
    # 1            7.8              0.88         0.00...
    # 2            7.8              0.76         0.04... 
    # 3           11.2              0.28         0.56...  
    # 4            7.4              0.70         0.00...
    
Not, to take a look at the data:
  
    print(ds.shape)
    (1599, 12)
    
Okay, now we know that we have 1599 observations and 12 variables/features, incluiding our target feature. Now, for printing summary statistics to get a better feeling of it:

    print(ds.describe())
    #        fixed acidity  volatile acidity  citric acid...
    # count    1599.000000       1599.000000  1599.000000...
    # mean        8.319637          0.527821     0.270976...
    # std         1.741096          0.179060     0.194801...
    # min         4.600000          0.120000     0.000000...
    # 25%         7.100000          0.390000     0.090000...
    # 50%         7.900000          0.520000     0.260000...
    # 75%         9.200000          0.640000     0.420000...
    # max        15.900000          1.580000     1.000000...
    
    Here's the list of all the features:
    quality (target)
    fixed acidity
    volatile acidity
    citric acid
    residual sugar
    chlorides
    free sulfur dioxide
    total sulfur dioxide
    density
    pH
    sulphates
    alcohol

All our features are numeric, but they all have different scales so make a note to remember to standarize later. We are cutting so much of the process of exploratory analysis, but let's move on to the next step because our focus is on *ML*.

## 2 - Split Data Into Train & Test Sets
Doing this at the beggining of the workflow is crucial for the sake of getting realistic stimates of the models performance.
First let's separate out target feature from our input features:

    y = ds.quality
    x = ds.drop('quality', axis = 1)
    
By doing so, it allows us to take advantage of the following function:

    x_train, x_test, y_train, y_test = train_test_split(x, y, 
                                                    test_size=0.2, 
                                                    random_state=123, 
                                                    stratify=y)
                                                    
We've just:
  1.  set aside 20% of the data for testing our model. 
  2.  set an arbitrary random state (seed) so that we can reproduce our results. 
  3.  This **is a really good practice**, and it is to **stratify our sample** by the target variable. This will ensure that our training set looks similar to our test set, making the evaluation metrics more reliable.  
  
## 3 - Declare Data Preprocessing steps
Remember that we said that we needed to standarize our data.
### What Is Standarization?
Is the process of subtracting the means from each feature and then dividing by the feature standard deviation. 

This is a common requirement for *ML* tasks because many algorithms assume that all features are centered around zero and have approximately the same variance.

#### Here is the code we WON'T USE:
It's pretty easy to scale a dataset with *Scikit-Learn*:

    x_train_scaled = preprocessing.scale(x_train)
    print(x_train_scaled)
    # array([[ 0.51358886,  2.19680282, -0.164433  , ...,  1.08415147,
    #         -0.69866131, -0.58608178],
    #        [-1.73698885, -0.31792985, -0.82867679, ...,  1.46964764,
    #          1.2491516 ,  2.97009781],
    #        [-0.35201795,  0.46443143, -0.47100705, ..., -0.13658641,
    # ...
    
We can confirm that the scaled dataset is centered at zero with **unit variance**:

    print(x_train_scaled.mean(axis=0))
    # [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
    print(x_train_scaled.std(axis=0))
    # [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]

So, why we won't use this code?

Because we won't be able to perform exactly the same transformation on our test set. We can scale the test set separatedly but we won't be using the same means and SDs as we used to transforming the trainig set (obviously because the means and SDs will be calculated on that 20% data that was leaved for testing, while the other means and SDs were calculated with the other 80% of the data).

This means that it wouldn't be a fair representation of how the model pipeline, including the preprocessing steps peroform on new data.

#### Here is the code we'll use:
Instead of directly incoking the scale function, we'll use a feature of *Scikit-Learn* called the **Transformer API.** This allows us to fit a preprocessing step using the training data the same way we would fit a model. Then, use the same transformation on future datasets! Here is the process:

  1.  Fit transformer on training set (saving means and SDs)
  2.  Apply transformer on training set (scaling training data)
  3.  Apply transformer to test set (using same means and SDs)
  
This will make our **final stimation of model performance** more realistic. And also will allow us to insert our preprocessing steps into a **cross-validation pipeline.** The code is the following:

    scaler = preprocessing.StandardScaler().fit(x_train)
    
We've just saved the means and SDs for each feature of the training set in the **scaler** object.
Remember that to confirm that worket we can perform an **unit variance** test:

    X_train_scaled = scaler.transform(X_train)
 
    print X_train_scaled.mean(axis=0)
    # [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]     <-- Problem here but let's continue for the sake of understanding the process

    print X_train_scaled.std(axis=0)
    # [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]
    
Notice how we ahre **using the scale object** here ton transform the training set. Later, we can transform the test set using the exact same means and SDs used on the training set:

    X_test_scaled = scaler.transform(X_test)
 
    print X_test_scaled.mean(axis=0)
    # [ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827
    #  -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]

    print X_test_scaled.std(axis=0)
    # [ 1.02160495  1.00135689  0.97456598  0.91099054  0.86716698  0.94193125
    #  1.03673213  1.03145119  0.95734849  0.83829505  1.0286218 ]
    
Here we find that our scaled features don't fit perfectly zero with unit variance. This is perfectly expected since we are using the SDs and means from the training set instead of the test set itself.

In reality, when we set up the **cross-validation pipeline** we will simply **declare the class object** instead of manually fit the **transformation API:**

    pipeline = make_pipeline(preprocessing.StandardScaler(), 
                             RandomForestRegressor(n_estimators=100))
                             
This is how exactly a **modeling cross-validation pipeline** looks like. It first transforms the data using *StandardScaler()* and then fits a model using a *random forest regressor*.

## 4- Declare Hyperparameters To Tune
Is time to consider the hyperparameters that we'll tune into our model.

### What are hyperparameters?
In the modeling process there are two types of parameters. 
    -   **Normal partameters:** Parameters that **CAN** be learned directly from the data (i.e. regression coefficients)
    -   ++Hyperparameters:** is the expression of high-level structural information about the model that **CAN'T** be learned from the data, and this is why they are tipically set before training the model.
    
Example: **Random Forest hyperparameters**

We will perform a *Random Forest Regression*. 

Within each decision tree, the computer can empirically decide where to create based on either: *mean-squared-error (MSE)* or *mean-absolute-error (MAE)*. This means that the actual branch location are **model paramenters.** And, since the model doesn't know which of the two criteria it should use, that's a **hyperparameter**. The algorithm also cannot decide how many tress to include in the forest, that's another **hyperparameter**. 

We can list our tunable hyperparameters with the following method:

    print pipeline.get_params()
    # ...
    # 'randomforestregressor__criterion': 'mse',
    # 'randomforestregressor__max_depth': None,
    # 'randomforestregressor__max_features': 'auto',
    # 'randomforestregressor__max_leaf_nodes': None,
    # ...
    
You can also find the list of all RF parameters on it's [documentation page.](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) When it's tuned through a pipeline, you'll need to prepend *randomforestgregressor__* before the parameter name, like in the code above.

Let's do that, let's declare the hyperparameters that we want to tune through **cross-validation.**

    hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],
                      'randomforestregressor__max_depth': [None, 5, 3, 1]}
                      
The format should be a python dictionary ({}) where the hyperparameters are the keys and the valeus are lists of settings to try. You can also find the options for parameters values on the documentation page. 

## 5 - Tune The Model Usign A Cross-Validation Pipeline
Cross-Validation is one of the MOST important skills in all of *ML* because it helps you to **maximize** model performance while **reducing** the chance of overfitting.

### What is Cross-Validation?
Is a process for reliably estimating the performance of a method for building a model by trainign and evaluating your model **multiple times** usign the same method. In this context, that method is **a set of hyperparameters**. The steps for doing cross-validation are the following:
    1.  Split your data into *k* equal parts, or folds. (tipically *k*=10)
    2.  Train your data in *k* - 1 folds. (all parts except a last one. i.e. 9 folds)
    3.  Evaluate it on the remaining fold. (i.e. the 10th fold)
    4.  Perform step 2 and 3 *k* times, each time holding out a **different** fold.
    5.  Aggregate the performance across all *k* folds. <- This is your performance metric.

Here is the K-Fold Cross-Validation diagram:

![KFoldCVDiagram](https://elitedatascience.com/wp-content/uploads/2016/12/K-fold_cross_validation_EN.jpg)

### Why is cross-validation so important in *ML*?
You want to train a *RF Regressor*. You must tune the hyperparamter of maximun depth allowed for each decision tree in your forest. 

How can you decide on this parameter?

There is where cross-validation is really helpful. Using only your training set, You can use cross-validation to evaluate different hyperparameters and stimate their effectiveness. 

By this, you keep your test set virgin and save it for the true hold-out final evaluation for selecting a model.

For example: you can cross-validate a *linear regression model*, a *random forest model*, and a *k-nearest neighbors* model using only the training set. Then you have your test set for making the final decision between all model families.

### What is a cross-validation pipeline?
The **best practice** when doing cross-validation, is to include your data preprocessing steps **inside** the cross-validation loop. This will prevent accidentally tainting your training folds with influential data from your test fold (which of couse will damage the final stimation of the model effectiveness).

Here is how the pipeline looks like when you have included your preprocessing steps:
    1.  Split your data into *k* equal parts.
    2.  **Preprocess *k* - 1 training folds.**
    3.  Train you model on the same *k* - 1 folds.
    4.  **Preprocess the hold-out fold with the same transformations from step 2.**
    5.  Evaluate your model on the same hold-out fold.
    6.  perform steps **from** 2 **to** 5 *k* times, each time holding out a different fold.
    7.  Aggregate the performance across all *k* folds. And you get your performance metric.
    
*Scikit-Learn* makes this process stupidly simple:
    
    clf = GridSearchCV(pipeline, hyperparameters, cv=10)
 
    # Fit and tune model
    clf.fit(X_train, y_train)
    
It's that easy. **GridSearchCV** performs *CV* across the entire grid (all possible permutations) of hyperparameters.

It takes in your model (in this case, a model pipeline), the hyperparamters you want to tune, and the number of folds to create.
The above is pseudo-code, there is more about writing cross-validation from scrath but that's not our focus here.

Now you can see the best parameters found on the *CV* process:

    print(clf.best_params_)
    {'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'log2'}
    
Looks like the default parameters for max depth and the log2 for max features win out for this dataset. 

*RF* models, in practice don't require much tuning. They tend to work pretty well out-of-the-box with a reasonable number of tress. 

The same steps above can be used when building any type of supervised learning model, in fact! Is a really good practice that helps you stimate the best parameters possible for the best output possible.

## 6 - Refit On The Entire Training Set
Once you've tuned your hyperparameters appropiately using *CV*, you can get a small improvement on the performance of the model by refitting it on the entire training set.

For our convenience, **GridSearchCV** from *sklearn* will automatically refit the model with the best set of hyperparameters usign the entire training set. This functionality is ON by default, you can check it by:

    print(clf.refit)
    True
    
Now, the *clf* object will be used at the model when applying it on other sets of data.

## 7 - Evaluate Model Pipeline On Test Data
This step is very straight forward once you **understand** that the *clf* object that you used to tune your hyperparameters with *CV*, can be used directly like a model object. 

To predict a new set of data:

    y_pred = clf.predict(X_test)

Now we can use the r^2score and mean squared error to evaluate our models performance:

    print r2_score(y_test, y_pred)
    # 0.45044082571584243

    print mean_squared_error(y_test, y_pred)
    # 0.35461593750000003
    
Is this performance good enough?

A rule of thumb is that your very first model won't probably br the best possible model. But there is a combination of three strategies that help you decide if you're satisfied with your model performance:
    1.  Start with the goal of the model. If the model is tied to a business proble, have you successfully solved the problem?
    2.  Look in academic literature to get a sense of the current performance benchmark for specific types of data.
    3.  Try to find low-hanging fruit in terms of ways to improve your model. (easy to prove ways to improve your efficiency)
    
There are various ways to improve a model. Here you have a few quick things that you can try:
    1.  Try other regression model families (e.g. regularized regression, boosted trees, etc.)
    2.  Collect more data of it's cheap to do so.
    3.  Engineer smarter features after espending more time on exploratory analysis.
    4.  Speak to a domain expert to get more context.
    
**NOTE:** when you try other families of models, is recommended to use the same training and test data as you used in your first model. That's the best way to get a true comparison between your models. 

## 8 - Save The Model For Future Use
Save your work so you can use the model in the future:

    joblib.dump(clf, 'rf_regressor.pkl')
That's it, when you want to use the model again simply use this function:

    clf2 = joblib.load('rf_regressor.pkl')
 
    # Predict data set using loaded model
    clf2.predict(X_test)
