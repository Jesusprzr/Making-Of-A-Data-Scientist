# Big Data & Data Mining
Here you use tools such as python, jupyter notebooks, hadoop, pandas (the python library), etc. To manipulate your data.

The Big Data definitions depends on you, there are two important attributes behind it:
- Size: Big Data is done with data in large database systems that are outside of the common data management tools.
- Usage: The techniques, models, patterns used in Big Data are pretty specific of it.

**Hadoop:**

The process:
The data is distributed through lots of servers (computers) running the same program, and this servers have each one a slice of the whole cluster of data. The server runs the program on it's individual part to finally send the results back, those are going to be sorted and redistributed to another process. The first process is the map process, and the second process is the reduce process.

These Big Data servers scale linearly.

Hadoop is an open source clone of the Google Big Data Architecture.

The tools of data sicence have been with us for decades, but the actual capability to create ML algorithms and combine them with our traditional tools (programming, databases, mathematics, statistics, probability, etc.) have open a new sea of possibilities for predicting outcomes, detecting patterns, engineering solutions, build optimization through data, etc.

Deep Learning is pretty new (less than 10 years old) and neural networks are pretty old (30+ years) but untill 2006 there wasn't much you could do with it. And now you have multi-layer neural networks that have massive improvements for what can be done in the field.
